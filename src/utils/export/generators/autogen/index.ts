import { ExportResult } from '../../types';
import { BaseExportGenerator } from '../base';

/**
 * AutoGen Export Generator.
 * Generates a Python project with Microsoft AutoGen multi-agent conversations.
 */
export class AutoGenGenerator extends BaseExportGenerator {
  generate(): ExportResult {
    // Validate first
    if (!this.validate()) {
      return this.buildResult();
    }

    // Options available for future use
    // const options = this.config.frameworkOptions.autogen!;

    // Generate project files
    this.addFile('README.md', this.generateReadme());
    this.addFile('pyproject.toml', this.generatePyProject());
    this.addFile('requirements.txt', this.generateRequirements());
    this.addFile('.env.example', this.generateEnvExample());

    // Generate config files
    this.addFile('config/llm_config.json', this.generateLlmConfig());
    this.addFile('config/agents.json', this.generateAgentsJson());
    this.addFile('config/group_chat.json', this.generateGroupChatJson());

    // Generate Python source files
    this.addFile('src/__init__.py', '# AutoGen workflow package\n');
    this.addFile('src/main.py', this.generateMain());
    this.addFile('src/orchestrator.py', this.generateOrchestrator());

    // Generate agent files
    this.generateAgentFiles();

    // Generate pattern files
    this.addFile('src/patterns/__init__.py', '');
    this.addFile('src/patterns/group_chat.py', this.generateGroupChatPattern());

    // Create workspace directory placeholder
    this.addFile('workspace/.gitkeep', '');

    return this.buildResult();
  }

  private generateReadme(): string {
    return `# ${this.config.name}

${this.config.description || 'AutoGen-based multi-agent workflow.'}

## Setup

\`\`\`bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # or venv\\Scripts\\activate on Windows

# Install dependencies
pip install -e .

# Set environment variables
cp .env.example .env
# Edit .env with your API keys
\`\`\`

## Usage

\`\`\`bash
# Run with group chat pattern (default)
python -m src.main "Your task or query here"

# Run with two-agent pattern
python -m src.main --pattern two-agent "Your task"

# Run with nested pattern
python -m src.main --pattern nested "Your task"
\`\`\`

## Project Structure

\`\`\`
.
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ llm_config.json     # LLM configurations
â”‚   â”œâ”€â”€ agents.json         # Agent definitions
â”‚   â””â”€â”€ group_chat.json     # Group chat settings
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py             # Entry point
â”‚   â”œâ”€â”€ orchestrator.py     # Main orchestration
â”‚   â”œâ”€â”€ agents/             # Agent implementations
â”‚   â””â”€â”€ patterns/           # Conversation patterns
â”œâ”€â”€ workspace/              # Agent workspace
â”œâ”€â”€ pyproject.toml
â””â”€â”€ requirements.txt
\`\`\`

---

*Generated by Visual Agent Builder on ${this.formatDate()}*
`;
  }

  private generatePyProject(): string {
    const name = this.slugify(this.config.name);
    return `[project]
name = "${name}"
version = "${this.config.version}"
description = "${this.config.description || 'AutoGen workflow'}"
readme = "README.md"
requires-python = ">=3.10"
dependencies = [
    "pyautogen>=0.2.0",
    "python-dotenv>=1.0.0",
]

[project.optional-dependencies]
dev = [
    "pytest>=8.0.0",
    "black>=24.0.0",
    "ruff>=0.3.0",
]

[project.scripts]
run = "src.main:main"

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"
`;
  }

  private generateRequirements(): string {
    return `pyautogen>=0.2.0
python-dotenv>=1.0.0
`;
  }

  private generateEnvExample(): string {
    return `# Required (at least one)
ANTHROPIC_API_KEY=your-anthropic-api-key
OPENAI_API_KEY=your-openai-api-key

# Optional
AZURE_OPENAI_API_KEY=your-azure-key
AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
`;
  }

  private generateLlmConfig(): string {
    const config = {
      config_list: [
        {
          model: this.config.defaultModel.model,
          api_key: '${ANTHROPIC_API_KEY}',
          api_type: 'anthropic',
          temperature: this.config.defaultModel.temperature,
          max_tokens: this.config.defaultModel.maxTokens,
        },
        {
          model: 'gpt-4-turbo-preview',
          api_key: '${OPENAI_API_KEY}',
          api_type: 'openai',
          temperature: 0.3,
          max_tokens: 4096,
        },
      ],
      filter_dict: {
        anthropic: { api_type: ['anthropic'] },
        openai: { api_type: ['openai'] },
      },
      cache_seed: 42,
      timeout: 120,
    };

    return JSON.stringify(config, null, 2);
  }

  private generateAgentsJson(): string {
    const agents = this.getNodesByType('AGENT');
    const options = this.config.frameworkOptions.autogen!;

    const agentConfigs: Record<string, any> = {
      user_proxy: {
        type: 'UserProxyAgent',
        name: 'user_proxy',
        human_input_mode: options.humanInputMode,
        max_consecutive_auto_reply: options.maxConsecutiveAutoReply,
        code_execution_config: {
          work_dir: 'workspace',
          use_docker: options.useDocker,
          timeout: 60,
        },
        system_message:
          'You are a helpful assistant that can execute code and interact with other agents.',
      },
    };

    for (const agent of agents) {
      const config = agent.data.config || {};
      const name = this.slugify(config.name || agent.data.label);

      agentConfigs[name] = {
        type: 'AssistantAgent',
        name: name,
        system_message:
          config.systemPrompt ||
          `You are ${config.name || agent.data.label}. ${config.description || ''}`,
        llm_config_key: 'anthropic',
        human_input_mode: 'NEVER',
      };

      // Add code execution for certain roles
      if (config.role === 'executor' || config.tools?.includes('Bash')) {
        agentConfigs[name].code_execution_config = {
          work_dir: 'workspace',
          use_docker: options.useDocker,
        };
      }
    }

    return JSON.stringify(
      {
        agents: agentConfigs,
        default_llm_config: {
          config_list_key: 'anthropic',
          cache_seed: 42,
          temperature: 0.3,
        },
      },
      null,
      2
    );
  }

  private generateGroupChatJson(): string {
    const agents = this.getNodesByType('AGENT');
    const agentNames = ['user_proxy', ...agents.map((a) =>
      this.slugify(a.data.config?.name || a.data.label)
    )];

    const config = {
      group_chat: {
        name: `${this.slugify(this.config.name)}_group`,
        agents: agentNames,
        max_round: 50,
        admin_name: 'user_proxy',
        speaker_selection_method: 'auto',
        allow_repeat_speaker: false,
        send_introductions: true,
      },
      group_chat_manager: {
        name: 'group_manager',
        llm_config_key: 'anthropic',
        system_message:
          'You are managing a group of AI agents. Coordinate their work efficiently.',
      },
    };

    return JSON.stringify(config, null, 2);
  }

  private generateMain(): string {
    return `"""
Main entry point for ${this.config.name} AutoGen workflow.
Generated by Visual Agent Builder.
"""

from __future__ import annotations

import argparse
from dotenv import load_dotenv

from .orchestrator import run_workflow


def main():
    """CLI entry point."""
    load_dotenv()

    parser = argparse.ArgumentParser(
        description="${this.config.description || 'AutoGen workflow'}"
    )
    parser.add_argument(
        "message",
        nargs="?",
        default="",
        help="Task or message to process"
    )
    parser.add_argument(
        "--pattern",
        choices=["two-agent", "group-chat", "nested"],
        default="group-chat",
        help="Conversation pattern to use"
    )
    parser.add_argument(
        "--verbose",
        action="store_true",
        help="Enable verbose output"
    )

    args = parser.parse_args()

    if not args.message:
        print("Error: Please provide a message or task")
        return 1

    print(f"\\nðŸ¤– Starting AutoGen workflow with pattern: {args.pattern}\\n")
    print(f"ðŸ“ Message: {args.message}\\n")
    print("=" * 60)

    result = run_workflow(
        message=args.message,
        pattern=args.pattern,
    )

    print("=" * 60)
    print("\\nâœ… Workflow completed!\\n")

    if "summary" in result:
        print("ðŸ“Š Summary:")
        print(result["summary"])

    return 0


if __name__ == "__main__":
    exit(main())
`;
  }

  private generateOrchestrator(): string {
    // Used by config generators, not directly in orchestrator template
    void this.getNodesByType('AGENT');

    return `"""
AutoGen orchestration for ${this.config.name}.
Generated by Visual Agent Builder.
"""

from __future__ import annotations

import json
import os
from pathlib import Path
from typing import Literal

import autogen
from autogen import AssistantAgent, UserProxyAgent, GroupChat, GroupChatManager


# =============================================================================
# Configuration Loading
# =============================================================================

CONFIG_DIR = Path(__file__).parent.parent / "config"
WORKSPACE_DIR = Path(__file__).parent.parent / "workspace"


def load_config(filename: str) -> dict:
    """Load JSON configuration file."""
    with open(CONFIG_DIR / filename) as f:
        content = f.read()
        # Expand environment variables
        for key, value in os.environ.items():
            content = content.replace(f"\\$" + "{" + key + "}", value)
        return json.loads(content)


def get_llm_config(key: str = "anthropic") -> dict:
    """Get LLM configuration by filter key."""
    config = load_config("llm_config.json")

    filter_dict = config.get("filter_dict", {}).get(key, {})
    config_list = autogen.filter_config(config["config_list"], filter_dict)

    return {
        "config_list": config_list,
        "cache_seed": config.get("cache_seed", 42),
        "timeout": config.get("timeout", 120),
    }


# =============================================================================
# Agent Factory
# =============================================================================

def create_agents() -> dict:
    """Create all agents from configuration."""
    agents_config = load_config("agents.json")
    agents = {}

    for name, config in agents_config["agents"].items():
        agent_type = config.pop("type")
        llm_config_key = config.pop("llm_config_key", "anthropic")

        llm_config = get_llm_config(llm_config_key) if agent_type != "UserProxyAgent" else False

        code_config = config.pop("code_execution_config", None)
        if code_config:
            code_config["work_dir"] = str(WORKSPACE_DIR)

        if agent_type == "UserProxyAgent":
            agents[name] = UserProxyAgent(
                name=config["name"],
                human_input_mode=config.get("human_input_mode", "TERMINATE"),
                max_consecutive_auto_reply=config.get("max_consecutive_auto_reply", 10),
                code_execution_config=code_config,
                system_message=config.get("system_message", ""),
            )
        else:
            agents[name] = AssistantAgent(
                name=config["name"],
                system_message=config["system_message"],
                llm_config=llm_config,
                human_input_mode=config.get("human_input_mode", "NEVER"),
                code_execution_config=code_config,
            )

    return agents


# =============================================================================
# Conversation Patterns
# =============================================================================

def run_two_agent(agents: dict, message: str) -> dict:
    """Run two-agent conversation."""
    user = agents["user_proxy"]
    # Get first non-user agent
    assistant = next(a for n, a in agents.items() if n != "user_proxy")

    chat_result = user.initiate_chat(assistant, message=message, max_turns=10)

    return {
        "history": chat_result.chat_history,
        "summary": chat_result.summary,
        "cost": chat_result.cost,
    }


def run_group_chat(agents: dict, message: str) -> dict:
    """Run group chat conversation."""
    group_chat_config = load_config("group_chat.json")

    agent_list = [agents[name] for name in group_chat_config["group_chat"]["agents"] if name in agents]

    group_chat = GroupChat(
        agents=agent_list,
        messages=[],
        max_round=group_chat_config["group_chat"]["max_round"],
        speaker_selection_method="auto",
    )

    manager = GroupChatManager(
        groupchat=group_chat,
        llm_config=get_llm_config("anthropic"),
    )

    user = agents["user_proxy"]
    chat_result = user.initiate_chat(manager, message=message)

    return {
        "history": chat_result.chat_history,
        "summary": chat_result.summary,
        "cost": chat_result.cost,
    }


def run_nested(agents: dict, message: str) -> dict:
    """Run nested conversation pattern."""
    results = {}
    user = agents["user_proxy"]

    # Sequential handoff through agents
    current_message = message
    agent_names = [n for n in agents.keys() if n != "user_proxy"]

    for i, name in enumerate(agent_names):
        agent = agents[name]
        result = user.initiate_chat(agent, message=current_message, max_turns=3)
        results[name] = result.summary
        current_message = f"Previous agent output: {result.summary}\\n\\nContinue with: {message}"

    return results


# =============================================================================
# Main Orchestrator
# =============================================================================

def run_workflow(
    message: str,
    pattern: Literal["two-agent", "group-chat", "nested"] = "group-chat",
) -> dict:
    """Run workflow with specified pattern."""

    # Ensure workspace exists
    WORKSPACE_DIR.mkdir(parents=True, exist_ok=True)

    agents = create_agents()

    if pattern == "two-agent":
        return run_two_agent(agents, message)
    elif pattern == "group-chat":
        return run_group_chat(agents, message)
    elif pattern == "nested":
        return run_nested(agents, message)
    else:
        raise ValueError(f"Unknown pattern: {pattern}")
`;
  }

  private generateAgentFiles(): void {
    const agents = this.getNodesByType('AGENT');

    this.addFile('src/agents/__init__.py', '');
    this.addFile(
      'src/agents/base.py',
      `"""
Base agent utilities.
Generated by Visual Agent Builder.
"""

from autogen import AssistantAgent, UserProxyAgent


def create_assistant(name: str, system_message: str, llm_config: dict) -> AssistantAgent:
    """Create an assistant agent."""
    return AssistantAgent(
        name=name,
        system_message=system_message,
        llm_config=llm_config,
    )


def create_user_proxy(name: str, code_execution: bool = False) -> UserProxyAgent:
    """Create a user proxy agent."""
    return UserProxyAgent(
        name=name,
        human_input_mode="TERMINATE",
        code_execution_config={"work_dir": "workspace"} if code_execution else False,
    )
`
    );

    for (const agent of agents) {
      const config = agent.data.config || {};
      const name = this.slugify(config.name || agent.data.label);
      const displayName = config.name || agent.data.label;

      const content = `"""
${displayName} agent implementation.
Generated by Visual Agent Builder.
"""

from autogen import AssistantAgent


SYSTEM_MESSAGE = """${(config.systemPrompt || `You are ${displayName}.`).replace(/"/g, '\\"')}"""


def create_${name}_agent(llm_config: dict) -> AssistantAgent:
    """Create and return the ${displayName} agent."""

    return AssistantAgent(
        name="${name}",
        system_message=SYSTEM_MESSAGE,
        llm_config=llm_config,
    )
`;

      this.addFile(`src/agents/${name}.py`, content);
    }
  }

  private generateGroupChatPattern(): string {
    return `"""
Group chat pattern implementation.
Generated by Visual Agent Builder.
"""

from autogen import GroupChat, GroupChatManager


def create_group_chat(agents: list, max_round: int = 50) -> GroupChat:
    """Create a group chat with the given agents."""
    return GroupChat(
        agents=agents,
        messages=[],
        max_round=max_round,
        speaker_selection_method="auto",
        allow_repeat_speaker=False,
        send_introductions=True,
    )


def create_manager(groupchat: GroupChat, llm_config: dict) -> GroupChatManager:
    """Create a group chat manager."""
    return GroupChatManager(
        groupchat=groupchat,
        llm_config=llm_config,
    )
`;
  }
}
